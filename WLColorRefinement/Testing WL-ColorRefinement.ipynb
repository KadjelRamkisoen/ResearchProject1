{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "# List of imports\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import dgl\n",
    "\n",
    "import WLColorRefinement as wl\n",
    "import CSL_data\n",
    "from data.molecules import MoleculeDataset\n",
    "from data.superpixels import SuperPixDataset\n",
    "from data.superpixels import SuperPixDatasetDGL\n",
    "from data.TUs import TUsDataset\n",
    "from data.TSP import TSPDataset\n",
    "from data.COLLAB import COLLABDataset\n",
    "from data.SBMs import SBMsDataset\n",
    "\n",
    "import create_reduced_graph as crg\n",
    "import analyse_reduction as ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the ZINC train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Loading dataset ZINC...\n",
      "train, test, val sizes : 10000 1000 1000\n",
      "[I] Finished loading.\n",
      "[I] Data load time: 9.6226s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Get ZINC graphs\n",
    "\"\"\"\n",
    "ZINC_graphs = MoleculeDataset('ZINC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Send the ZINC graphs to the coloring function\n",
    "\"\"\"\n",
    "ZINC_colored_graphs = list()\n",
    "\n",
    "for graph in ZINC_graphs.train:\n",
    "    ZINC_colored_graphs.append(wl.wl_coloring(graph.__getitem__(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get ZINC reduced graphs\n",
    "\"\"\"\n",
    "ZINC_reduced_graphs = crg.reduced_graph(ZINC_colored_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Save the ZINC colored and reduced graphs \n",
    "\"\"\"\n",
    "os.chdir('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\molecules')\n",
    "\n",
    "with open('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\molecules\\\\ZINC_train_colored.pkl','wb') as f:\n",
    "            pickle.dump([ZINC_colored_graphs],f)\n",
    "with open('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\molecules\\\\ZINC_train_reduced.pkl','wb') as f:\n",
    "            pickle.dump([ZINC_reduced_graphs],f)\n",
    "        \n",
    "os.chdir('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the CSL train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get CSL graphs\n",
    "\"\"\"\n",
    "CSL_graphs = CSL_data.get_CSL_graphs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Send the CSL graphs to the coloring function\n",
    "\"\"\"\n",
    "CSL_colored_graphs = list()\n",
    "\n",
    "for i in CSL_graphs:\n",
    "    CSL_colored_graphs.append(wl.wl_coloring(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the TU train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get TU graphs\n",
    "\"\"\"\n",
    "Collab_graph = COLLABDataset('OGBL-COLLAB').graph\n",
    "ENZYMES_graphs = TUsDataset('ENZYMES')\n",
    "DD_graphs = TUsDataset('DD')\n",
    "PROTEINS_full_graphs= TUsDataset('PROTEINS_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Send the TU graphs to the coloring function\n",
    "\"\"\"\n",
    "PROTEIN_colored_graphs = list()\n",
    "for graph in PROTEINS_full_graphs.train:\n",
    "    PROTEIN_colored_graphs.append(wl.wl_coloring(graph.__getitem__(0)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reducing the MNIST train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Loading dataset MNIST...\n",
      "train, test, val sizes : 55000 5000 10000\n",
      "[I] Finished loading.\n",
      "[I] Data load time: 56.5733s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Get MNIST graphs\n",
    "\"\"\"\n",
    "MNIST_graphs = SuperPixDataset('MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Send the MNIST graphs to the coloring function\n",
    "\"\"\"\n",
    "MNIST_colored_graphs = list()\n",
    "\n",
    "for graph in MNIST_graphs.train:\n",
    "    MNIST_colored_graphs.append(wl.wl_coloring(graph.__getitem__(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get MNIST reduced graphs\n",
    "\"\"\"\n",
    "MNIST_reduced_graphs = crg.reduced_graph(MNIST_colored_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Save the MNIST colored and reduced graphs \n",
    "\"\"\"\n",
    "os.chdir('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\superpixels')\n",
    "\n",
    "with open('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\superpixels\\\\MNIST_train_reduced.pkl','wb') as f:\n",
    "            pickle.dump([MNIST_colored_graphs],f)\n",
    "        \n",
    "with open('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\superpixels\\\\MNIST_train_reduced.pkl','wb') as f:\n",
    "            pickle.dump([MNIST_reduced_graphs],f)\n",
    "        \n",
    "os.chdir('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_reduction_data('CompressionMNIST.csv', MNIST_graphs, MNIST_reduced_graphs[0])\n",
    "create_scatter_plot(pd.read_csv (r'CompressionMNIST.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the CIFAR10 train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Loading dataset CIFAR10...\n",
      "train, test, val sizes : 45000 10000 10000\n",
      "[I] Finished loading.\n",
      "[I] Data load time: 46.3957s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Get CIFAR10 graphs\n",
    "\"\"\"\n",
    "CIFAR10_graphs = SuperPixDataset('CIFAR10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Send the CIFAR10 graphs to the coloring function\n",
    "\"\"\"\n",
    "CIFAR10_colored_graphs = list()\n",
    "\n",
    "for graph in CIFAR10_graphs.train:\n",
    "    CIFAR10_colored_graphs.append(wl.wl_coloring(graph.__getitem__(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get CIFAR10 reduced graphs\n",
    "\"\"\"\n",
    "CIFAR10_reduced_graphs = crg.reduced_graph(CIFAR10_colored_graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Save the CIFAR10 colored and reduced graphs \n",
    "\"\"\"\n",
    "os.chdir('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\SBMs')\n",
    "   \n",
    "with open('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\superpixels\\\\CIFAR10_train_reduced.pkl',\"rb\") as f:\n",
    "    pickle.dump([CIFAR10_colored_graphs], f)\n",
    "    \n",
    "with open('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\superpixels\\\\CIFAR10_train_reduced.pkl',\"rb\") as f:\n",
    "    pickle.dump([CIFAR10_reduced_graphs], f)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_reduction_data('CompressionPATTERN.csv', SBM_PATTERN_graphs, PATTERN_reduced_graphs)\n",
    "create_scatter_plot(pd.read_csv(r'CompressionPATTERN.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the SBM train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get SBM graphs\n",
    "\"\"\"\n",
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "SBM_CLUSTER_graphs = SBMsDataset('SBM_CLUSTER')\n",
    "\n",
    "SBM_PATTERN_graphs = SBMsDataset('SBM_PATTERN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Send the SBM graphs to the coloring function\n",
    "\"\"\"\n",
    "SBM_PATTERN_colored_graphs = list()\n",
    "SBM_CLUSTER_colored_graphs = list()\n",
    "\n",
    "for graph in SBM_CLUSTER_graphs.train:\n",
    "    SBM_CLUSTER_colored_graphs.append(wl.wl_coloring(graph.__getitem__(0)))\n",
    "    \n",
    "for graph in SBM_PATTERN_graphs.train:\n",
    "    SBM_PATTERN_colored_graphs.append(wl.wl_coloring(graph.__getitem__(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get the SBM reduced graphs\n",
    "\"\"\"\n",
    "SBM_CLUSTER_reduced_graphs = crg.reduced_graph(SBM_CLUSTER_colored_graphs[])\n",
    "SBM_PATTERN_reduced_graphs = crg.reduced_graph(SBM_PATTERN_colored_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Save the SBM colored and reduced graphs \n",
    "\"\"\"\n",
    "os.chdir('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\SBMs')\n",
    "\n",
    "with open('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\SBMs\\\\PATTERN_train_colored.pkl','wb') as f:\n",
    "            pickle.dump([SBM_PATTERN_colored_graphs],f)\n",
    "        \n",
    "with open('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\SBMs\\\\PATTERN_train_colored.pkl','wb') as f:\n",
    "            pickle.dump([SBM_PATTERN_reduced_graphs],f)\n",
    "\n",
    "        \n",
    "with open('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\SBMs\\\\CLUSTER_train_colored.pkl','wb') as f:\n",
    "            pickle.dump([SBM_CLUSTER_colored_graphs],f)\n",
    "        \n",
    "with open('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement\\\\data\\\\SBMs\\\\CLUSTER_train_reduced.pkl','wb') as f:\n",
    "            pickle.dump([SBM_CLUSTER_reduced_graphs],f)\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\User1\\\\Documents\\\\GitHub\\\\ResearchProject1\\\\WLColorRefinement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def store_reduction_data(file, original_graphs, reduced_graphs):\n",
    "    i = 0\n",
    "    \n",
    "    with open(file, mode='w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        fieldnames = ['Graph nr', \n",
    "                      'Original nodes', 'Reduced nodes', 'Node compr. rate', \n",
    "                      'Original edges', 'Reduced edges', 'Edge compr. rate'\n",
    "                     ]\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(fieldnames)\n",
    "\n",
    "        while i < len(reduced_graphs):\n",
    "            print('Graph ', i)\n",
    "            x = original_graphs.train.__getitem__(i)[0]\n",
    "            y = reduced_graphs[i]\n",
    "#             nx_x = x.to_networkx()\n",
    "#             nx_y = y.to_networkx()\n",
    "#             pos_x = nx.kamada_kawai_layout(nx_x)\n",
    "#             pos_y = nx.kamada_kawai_layout(nx_y)\n",
    "\n",
    "            writer.writerow([i,x.number_of_nodes(), \n",
    "                             y.number_of_nodes(), \n",
    "                             round((((x.number_of_nodes() - y.number_of_nodes())/x.number_of_nodes())*100),2),\n",
    "                             x.number_of_edges(), \n",
    "                             x.number_of_edges(),\n",
    "                             round((((x.number_of_edges() - y.number_of_edges())/x.number_of_edges())*100),2)\n",
    "                            ])\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "            \n",
    "def create_scatter_plot(df):\n",
    "    print('Analysis')\n",
    "    print('\\n')\n",
    "    \n",
    "    node = df[df.columns[3]]\n",
    "    edge = df[df.columns[6]]\n",
    "\n",
    "    plt.scatter(node, edge, s = 1)\n",
    "    plt.show()\n",
    "    \n",
    "#     edge.plot.hist(grid=True, bins=20, rwidth=0.9)\n",
    "#     node.plot.hist(grid=True, bins=20, rwidth=0.9)\n",
    "    \n",
    "#     i = 0\n",
    "#     count_node = {\"<=10\":0,\n",
    "#             \"<=20\":0,\n",
    "#             \"<=30\":0,\n",
    "#             \"<=40\":0,\n",
    "#             \"<=50\":0,\n",
    "#             \"<=60\":0,\n",
    "#             \"<=70\":0,\n",
    "#             \"<=80\":0,\n",
    "#             \"<=90\":0,\n",
    "#             \"<=100\":0\n",
    "#             }\n",
    "    \n",
    "#     count_edge = {\"<=10\":0,\n",
    "#             \"<=20\":0,\n",
    "#             \"<=30\":0,\n",
    "#             \"<=40\":0,\n",
    "#             \"<=50\":0,\n",
    "#             \"<=60\":0,\n",
    "#             \"<=70\":0,\n",
    "#             \"<=80\":0,\n",
    "#             \"<=90\":0,\n",
    "#             \"<=100\":0\n",
    "#             }\n",
    "#     while i < len(node):\n",
    "#         if node[i] <= 10:\n",
    "#             count_node[\"<=10\"] += 1\n",
    "#         elif node[i] <= 20:\n",
    "#             count_node[\"<=20\"] += 1\n",
    "#         elif node[i] <= 30:\n",
    "#             count_node[\"<=30\"] += 1\n",
    "#         elif node[i] <= 40:\n",
    "#             count_node[\"<=40\"] += 1\n",
    "#         elif node[i] <= 50:\n",
    "#             count_node[\"<=50\"] += 1\n",
    "#         elif node[i] <= 60:\n",
    "#             count_node[\"<=60\"] += 1\n",
    "#         elif node[i] <= 70:\n",
    "#             count_node[\"<=70\"] += 1\n",
    "#         elif node[i] <= 80:\n",
    "#             count_node[\"<=80\"] += 1\n",
    "#         elif node[i] <= 90:\n",
    "#             count_node[\"<=90\"] += 1\n",
    "#         elif node[i] <= 100:\n",
    "#             count_node[\"<=100\"] += 1\n",
    "#         i += 1\n",
    "#     i = 0\n",
    "#     while i < len(edge):\n",
    "#         print\n",
    "#         if edge[i] <= 10:\n",
    "#             count_edge[\"<=10\"] += 1\n",
    "#         elif edge[i] <= 20:\n",
    "#             count_edge[\"<=20\"] += 1\n",
    "#         elif edge[i] <= 30:\n",
    "#             count_edge[\"<=30\"] += 1\n",
    "#         elif edge[i] <= 40:\n",
    "#             count_edge[\"<=40\"] += 1\n",
    "#         elif edge[i] <= 50:\n",
    "#             count_edge[\"<=50\"] += 1\n",
    "#         elif edge[i] <= 60:\n",
    "#             count_edge[\"<=60\"] += 1\n",
    "#         elif edge[i] <= 70:\n",
    "#             count_edge[\"<=70\"] += 1\n",
    "#         elif edge[i] <= 80:\n",
    "#             count_edge[\"<=80\"] += 1\n",
    "#         elif edge[i] <= 90:\n",
    "#             count_edge[\"<=90\"] += 1\n",
    "#         elif edge[i] <= 100:\n",
    "#             count_edge[\"<=100\"] += 1\n",
    "#         i += 1\n",
    "#     print('nodes reduction\\n', count_node)\n",
    "#     print('edges reduction\\n', count_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_reduction_data('CompressionMNIST.csv', MNIST_graphs, MNIST_reduced_graphs[0])\n",
    "analyse_data(pd.read_csv (r'CompressionMNIST.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_reduction_data('CompressionPATTERN.csv', SBM_PATTERN_graphs, PATTERN_reduced_graphs)\n",
    "analyse_data(pd.read_csv (r'CompressionPATTERN.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_reduction_data('CompressionCLUSTER.csv', SBM_CLUSTER_graphs, CLUSTER_reduced_graphs)\n",
    "analyse_data(pd.read_csv (r'CompressionCLUSTER.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_reduction_data('CompressionMolecules.csv', ZINC_graphs, ZINC_reduced_graphs)\n",
    "# analyse_data(pd.read_csv (r'CompressionMolecules.csv'))\n",
    "\n",
    "# store_reduction_data('CompressionCIFAR10.csv', CIFAR10_graphs, CIFAR10_reduced_graphs)\n",
    "# analyse_data(pd.read_csv (r'CompressionCIFAR10.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
